%
% File: chap03.tex
% Description: Related Work chapter 
%
\let\textcircled=\pgftextcircled
\chapter{Related Work}
\label{chap:RelatedWork}
\initial{I}n this chapter the related work is presented. Selective Replication, a branch in soft error detection is first briefly introduced.  Current solutions for soft error management are discussed, their strengths and their weaknesses are analyzed. Multiple schemes of soft error detection via software are discussed, in order to establish the current thesis approach. Finally the chapter ends trying to revise the use of hyper-threading in error detection techniques. 

As it was mentioned before, there are many ways to accomplish soft error detection via hardware, but they all require specialized hardware, which is out of the scope of the current thesis. As an alternative, different software-only fault-tolerance mechanisms exist. Since common hardware is used, they need to modify the program, either after compilation by binary instrumentation or during compilation. These solutions, are categorized into Instruction Level Redundancy (thread-local) and Redundant Multi-Threading error detections. In the first category, redundancy is added in the same thread and in the latter replication is distributed in two threads. 

\section{Selective Replication}
\label{sec:SelectiveReplication}

One important aspect about bit flips it that they do not necessarily lead to a system failure. For that, many authors have explored the possibility of protecting just the parts of the application that when hit by a particle strike, may result in data corruption. Protecting only areas of the program susceptible to soft errors is usually referred to Selective Replication. Calhoun \textit{et al} in \cite{calhoun2017towards} try to determine the way a soft error propagates thought different kinds of HPC applications. They say that in order to understand what parts of the applications needs protection, it is first important to know how a soft error spreads to contaminate more data or instructions. The authors perform their tests with MPI applications and analyze fault propagation at two levels. The first one is rank local, how the soft error modifies local values in the same process where it happened; the second one is among ranks, the way other process are contaminated when inter-process communication occurs. 

Shoestring \cite{feng2010shoestring} accomplishes fault tolerance based on two areas of prior research: symptom-based fault detection and redundancy through ILR. The former one relies on the fact that applications in presence of transient faults often exhibit abnormal behavior. Examples of these symptoms are branch mispredictions or memory access exceptions. They say that symptom-based detection is inexpensive but the amount of coverage is typically limited. For that they take advantage of ILR. Which can get to nearly 100\% of fault coverage, however replicating the application incurs a lot of performance overhead. The authors perform compiler analysis to identify the vulnerable instructions that symptom-based fault detection fails to detect and protect them with redundancy. One downside of the solution is that the algorithm to identify such instructions requires hardware specific knowledge, for example knowledge about exception-throwing instructions from the instruction set architecture \cite{laguna2016ipas}.

IPAS \cite{laguna2016ipas} also attempts to protect only the code that actually needs coverage. The authors use a machine learning process in order to identify the instructions that require duplication. They claim that protecting the whole application wastes a lot of resources due to the low probability of a soft error actually modifying important data. And even in such cases, it may not be catastrophic because in most HPC applications a soft error can be masked due to the algorithm properties (meaning that the result is acceptable even in the presence of an error). With a well-trained model, they manage to get low slowdown achieving high coverage percentage against data corruption. One downside on the solution is the time it takes the machine learning process to produce accurate results, which is considerable slower than a compiler based approach like Shoestring \cite{feng2010shoestring}. 

Regardless of what needs to be protected in the application, the replication phase must be efficient. For that, the next two sections explain further replications mechanisms. 

\section{Instruction Level Redundancy}
\label{sec:InstructionLevelRedundancy}
A well known thread-local replication compiler-based approach is SWIFT [13], where instructions are duplicated within the same thread and periodic checks of both results are added, ensuring the correct execution of single-threaded applications. The resulting code needs two times as many registers as the original code, which can potentially cause register spills (using RAM when there is no more space available in the processor). For that, in architectures such as Itanium with many registers, the overhead of SWIFT is low \cite{zhang2012daft}. This scheme provides only fault detection, though a common checkpointing technique can be integrated with SWIFT for full soft-error protection. 

HAFT (Hardware Assisted Fault Tolerance) provides full soft error management. It uses ILR for fault detection and Intel's version of \textit{Hardware Transactional Memory}, Transactional Synchronization Extensions (TSX), for fault correction \cite{kuvaiskii2016haft}. Hardware Transactional Memory provides a mutual exclusion mechanism, in which there is a way to create an atomic block of instructions, a transaction. When executing the transaction, modified data is kept temporarily in the core's cache. If no memory conflict (read/write, write/write) is detected with another concurrent transaction, every instruction in the block is atomically committed to RAM; so every other core can view the results. On the other hand, when a memory collision is identified, all modified data stored in the cache is reverted and no single operation of the transaction is committed to RAM \cite{herlihy1993transactional}. 

Intel's TSX provides a very convenient way to explicitly force a transaction abort and therefore restore the state of the core to the beginning of the transaction. Such mechanism is exploited in HAFT for soft error recovery purposes \cite{kuvaiskii2016haft}. After the code has been duplicated with ILR, the application is wrapped in HTM-based transactions in order provide recovery. When an error is detected by the ILR checks, the transaction is explicitly aborted, the state of the application is restored before the transaction began and the execution is retried.

Sadly, since Intel TSX was not thought to be a soft error recovery mechanism, a transaction may fail due to several (sometimes unexpected) reasons. Hence HAFT presents a best effort approach, where a transaction is retried a fixed number of times. If all such attempts fail, that part of the application runs again without HTM. That means, if an error occurs in this unprotected moment of execution, ILR has no choice but to permanently abort the program. Mostly this design choice in HAFT is driven by the restrictions that Intel TSX currently exhibits \cite{kuvaiskii2016haft}. %The protected application in general, based on their results, runs about 100\% slower than the unprotected version, that means 2x times the original application. 

%Another reference where HTM is used as a recovery mechanism is Fault Tolerant Execution on COTS Multi-core Processors with Hardware Transactional Memory Support \cite{haas2017fault}. It is an approach which leverages Intel TSX to support implicit checkpoint creation and fast rollback. The authors combine a software-based redundant execution for detecting faults with hardware transactional memory to restore the state of the application if necessary. The main idea of the paper is to redundantly execute each user process and to instrument signature-based comparison on function level. They prefer processes instead of redundant threads because the virtual memory management of the operating system guarantees physical memory isolation and therefore if one error occurs in one process it is less likely that will propagate to its duplicate. This is true for local memory to the process, but if the error happens with a non-volatile memory access (which can execute some I/O operation), the fact that they use processes instead of threads does not help at all. This proposal resembles a lot redundant multi-threading approaches, but using processes. Generally speaking, on each function boundary, a signature is created which identifies the block of code. It is then shared to the duplicated process so it can be compared. Both signatures should match and if they don't, the recovery mechanism is initiated \cite{haas2017fault}. 

%Since they rely on Intel's TSX for recovery purposes, the duplicated process (process 2) has hardware memory transactions. The flow of the technique goes like this: the main process executes the first function of the program, it creates the signature, shares it with its duplicate and keeps running. When the process two receives the signature, it starts a transaction, executes the function and at the end compares the two signatures, if it detects a mismatch it commences the recovery. This unsynchronized scheme is due that signature exchange within a transaction always results in an abort, due to conflicting memory accesses. Since only the duplicated process has TSX, the main one continues executing the application and consequently is a couple of functions ahead of the other one. If an error is encountered, the duplicated process aborts its transaction, but the main one is already some steps ahead and cannot be rolled back to the desired point; for that, the authors decide to kill the main process and fork a new one \cite{haas2017fault}. 

%The average performance overhead, according to their result, of the technique is 140\%. The authors claim that with more hardware support of some current features, the overhead could be further decreased significantly. They propose in what ways the hardware support could be expanded, they simulate this scenarios and get to the conclusion that the overhead would be as low as a 47\% on average. Since these last tests are simulations based on hardware features that do not exist yet, the original performance overhead is the one that really counts. 

%Is it at the end not a scheduling problem? The best scenario would be sort like a round robin scheduling where each quantum is exactly how much the producer takes to get to a waiting instruction and how much the consumer takes to produce the signal. 

%What about if we simulate the threads? like in Advanced Operating Systems project 1, then there would not be false sharing cache. Will the cost of managing the pseudo-threads will be bigger than the advantage of completely sharing the L1.... hmmm but then we would not be using the shared architectural state which is like the best advantage of hyper-threading.

\section{Redundant Multi-Treading}
\label{sec:RedundantMultiTreading}

One example of redundant multi-threading with asynchronous communication pattern is COMET (Communication-Optimised Multi-threaded Error-detection Technique) \cite{mitropoulou2016comet}. Mitropoulou \textit{et al} first identify that the performance overhead of most redundant multi-threading techniques lies in poorly executed inter-thread communication. Since the two threads need to exchange data frequently, if this is naively implemented it can incur significant performance overhead. They propose several code optimizations on the generated code in order to alleviate the problem, but mostly they rely on the Lynx queue \cite{mitropoulou2016lynx}. The Lynx queue is a \textit{Multi Section} Single Producer/Single Consumer (SP/SC) with fast enqueue/dequeue operations. Such queues are divided into sections and only one thread is allowed to access a section at a time. The sections have a state indicating who is using it: the producer by writing values or the consumer reading already produced fields. The synchronization of threads happens only at section boundaries, the consumer cannot start reading from a section that has not been totally written; and the producer cannot start filling a section that has not been entirely read (except the first time). This separation allows that both threads access the queue simultaneously without locking each other, provided they work on different sections. A multi-section queue tries to solve problems such as cache ping-pong and false sharing \cite{mitropoulou2016lynx}.

The main novelty of Lynx queue is using just two instructions per enqueue and dequeue operations; that is the read/write of the data and advancing the dequeue/enqueue pointer. This is performed by taking advantage of memory protection systems available in commodity processors and operating systems. Each section of the queue is followed by a protected memory zone, non-readable and non-writable, which are called red zones. This red zones serve as the synchronization mechanism triggers. Every time the enqueue/dequeue pointer reaches a red zone at the end of the section, it will access protected memory. Therefore, a segmentation fault signal is raised which is managed by a custom exception handler where the synchronization takes place. By doing this trick, they remove the synchronization cost from the critical path of execution and reduce the overall number of instructions executed \cite{mitropoulou2016comet} \cite{mitropoulou2016lynx}. 

The authors from COMET actually alter the Lynx queue in order to make it more efficient for the redundant multi-threading case. They sacrifice general use of the modified queue for performance gains. The error detection code generation is performed automatically as a RTL \footnote{RTL stands for Register Transfer Language. The last part of the GCC compiler work is done in this low-level intermediate representation \cite{GCC}} pass in the back-end of GCC. 

While the Lynx Queue helps a lot in the inter-thread communication, the fact that is a multi-section queue means that the communication pattern is asynchronous, because the producer never awaits for the consumer check. That is done in this case to avoid deadlocks. If the producer has to pause for the consumer but is in the middle of filling its section, it will wait forever, since the consumer cannot start reading values of such section until it is completely filled. Having this scenario means that there is no protection against volatile memory accesses. In fact, the authors from \cite{mitropoulou2016comet} do recognize that such operations are protected in the state of the art solutions, but then they do not mention mention how this problem is solved in their paper.  

Decoupled Acyclic Fault Tolerance (DAFT) \cite{zhang2012daft} is another example of the redundant multi-threading schemes, where the threads are scheduled to different cores. The authors present a non-speculative version with synchronous communication pattern. Before any memory operation in the leading thread, the address and value are sent to the trailing thread, compared against the corresponding duplicate values and then a check is sent back to the leading thread. While all this happens, the leading thread has to wait (by spinning) for such check before continuing. In this version of DAFT, there is a lot of performance overhead because of the inter-thread communication. % (about 3x). 

The authors then present a speculative version with a semi-synchronous communication pattern. Basically, the idea is to speculate that the operations performed by the leading thread are without-errors and therefore allowing it to continue freely as much as possible. That permits the leading thread to advance on some instructions without a proof of correctness, removing the busy waiting from the critical path of the application and also reducing communication bandwidth. Zhang \textit{et al} (authors from DAFT) point out that volatile variable accesses still need some kind of check before execution, in order to avoid catastrophic effects. So, they implement ILR in the leading thread only for volatile memory accesses and their dependencies. For any other operation, the leading thread is allowed to make forward progress, producing values that will be later checked by the consumer, but not having to wait for them \cite{zhang2012daft}.  

However, the approach comes with the penalty of possibly having an incorrect execution of the program due to misspeculation. Since the leading thread is allowed to continue on some operations without a check, it might be unaware that a transient fault has already corrupted important data. The authors try to solve this issue by injecting artificial handlers in the code, so in case an exception is raised by the leading thread on operations that were speculated to be correct, they can be properly managed. There is another downside of the solution, the fact that there might be several dependencies for volatile memory accesses, that could depend as well on other values. That situation will  cause the leading thread to have ILR in a lot of its code; which would defeat the whole purpose of having a redundant multi-threading approach, this has already been discussed on Section \ref{subsec:BacgroundRedundantMultiThreading}. By performing such mechanisms and other optimizations, DAFT manages to still be deliver a correct execution of the program and decrease performance overhead significantly compared to their non-speculative version \cite{zhang2012daft}. 

%to 1.38x on average \cite{zhang2012daft}, which is to our extent the lowest among the available solutions.

Another Redundant Multi-Threading approach is presented in \textit{Compiler-managed software-based redundant multi-threading for transient fault detection} \cite{wang2007compiler}. As well as DAFT, they analyzed the high cost of having a synchronous communication pattern and conclude it involves too much performance overhead to be a manageable solution. The authors, also identify that volatile memory accesses must be confirmed correct before the leading thread can execute them. In order to detect such operations, they rely on variable attributes available to the compiler and only in those cases the leading thread awaits for the trailing thread's confirmation. Basically, they propose another semi-synchronous communication pattern that only synchronizes with the trailing thread on volatile memory accesses. Sadly, they ran into the same problems DAFT does when allowing the leading thread to continue on some operations without a check. Sometimes exceptions might be triggered due to soft errors and the authors decide (as well as in DAFT) to install artificial handlers on the application in order to deal with those signals. 

Both DAFT and \textit{Compiler-managed software-based redundant multi-threading for transient fault detection}, propose a Redundant Multi-Threading approach with semi-synchronous communication pattern. The difference between them is how they deal with memory volatile accesses. The latter option, makes the leading thread wait for a confirmation from the trailing thread in such cases. On the other hand, DAFT decides to implement ILR in the leading thread instead of having to synchronize both threads. Both options run into problems when they choose to allow the leading thread continue on some operations without a proof of correctness. 

COMET \cite{mitropoulou2016comet} focuses on optimizing the inter-thread communication in a redundant multi-threading approach with asynchronous communication pattern. They compare their results against the state of the art solutions, or so they claim, and get 31.4\% higher performance on average by reducing the number of instructions executed. However, they actually perform the tests against a technique similar to DAFT \cite{zhang2012daft} and \textit{Compiler-managed software-based redundant multi-threading for transient fault detection} \cite{wang2007compiler}, which both have semi-synchronous communications patterns. But, since COMET does not protect against volatile variable accesses because of its asynchronous communication, it seems that is not a fair comparison. 

\subsection{Use of Hyper-Threading In Soft Error Detection}
\label{subsec:useOfHyperThreading}
The use of Hyper-Threading in Soft Error detection techniques in general has, to our extent, not been investigated properly. There are however, some papers published a while ago (2000 and 2002) that propose hardware solutions leveraging from a Simultaneous Multi-Threading (SMT) Processor. \textit{Transient fault detection via simultaneous multithreading} \cite{reinhardt2000detectionSimultaneousMultithreading}, suggest how a SMT processor can be tweaked and improved, in order to be able to create a Simultaneous and Redundantly Threaded (SRT) processor.  A SRT processor would be able to efficiently execute an application with redundancy in order to detect soft errors.

\textit{Transient-fault recovery using simultaneous multithreading} \cite{vijaykumar2002recoverySimultaneousMultithreading}, continues the work done by Reinhardt \textit{et al} \cite{reinhardt2000detectionSimultaneousMultithreading} to extend a SRT processor to a Simultaneously and Redundantly Threaded processor with Recovery (SRTR). The authors propose how would be best to add error recovery to a STR processor. Both solutions are hardware-based, which are out of the scope of the current thesis. 


Specifically on Redundant Multi-Threading approaches, Hyper-Threading has neither been investigated appropriately. The only mention we know of is in \textit{Compiler-managed software-based redundant multi-threading for transient fault detection} \cite{wang2007compiler}, in a single experiment with their semi-synchronous communication pattern. They authors compare hyper-threads that share the L1 cache (config 1), against threads on different cores within the same cluster that share a L4 cache (config 2) and threads on different clusters with different L4 caches (config 3). The physical configuration of their test machine made the second configuration perform best, the configuration 1 came in second place (but not by much) and the configuration 3 behaving much worse than the other two. It is important to notice that such paper was published in the beginning of 2007, more than 10 years ago. For that, and other reasons explained in Section \ref{sec:justification}, we believe that a more detailed investigation of Hyper-Threading in Redundant Multi-Threading soft-error detection techniques is necessary. 
